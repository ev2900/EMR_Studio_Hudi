{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6e155b",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebcf3b",
   "metadata": {},
   "source": [
    "Before running the code in the cell(s) below SSH into your EMR cluster and run the following \n",
    "\n",
    "```hdfs dfs -mkdir -p /apps/hudi/lib```\n",
    "\n",
    "```hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar```\n",
    "\n",
    "```hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar```\n",
    "\n",
    "This will copy the Hudi jar files from the local file system to HDFS on the master node of the notebook cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87122ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'proxyUser': 'assumed-role_admin_sharkech-Isengard', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a7f2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c28e2303e24788bd29969d919e250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>19</td><td>application_1638389236079_0047</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-46-53.ec2.internal:20888/proxy/application_1638389236079_0047/\" class=\"emr-proxy-link\" emr-resource=\"j-1FP19FCTD2K8Z\n",
       "\" application-id=\"application_1638389236079_0047\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-44-248.ec2.internal:8042/node/containerlogs/container_1638389236079_0047_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0bd93",
   "metadata": {},
   "source": [
    "## Write to S3 via. Hudi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea975cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd45dc2b3814faa81f2f0b257728586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "        (\"1\", \"Chris\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"2\", \"Will\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"3\", \"Emma\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"4\", \"John\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"5\", \"Eric\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"6\", \"Adam\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False), \n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "inputDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# Create hudiOptions variable\n",
    "hudiOptions = {\n",
    "    'hoodie.table.name': 'copy_on_write_python',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'create_date',\n",
    "    'hoodie.datasource.write.precombine.field': 'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.enable': 'true',\n",
    "    'hoodie.datasource.hive_sync.table': 'copy_on_write_python',\n",
    "    'hoodie.datasource.hive_sync.partition_fields': 'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "    'hoodie.datasource.write.table.type': 'MERGE_ON_READ'\n",
    "}\n",
    "\n",
    "# Write a DataFrame to S3 as a Hudi dataset \n",
    "inputDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'insert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('s3://hudi-sharkech/merge_on_read_python/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac5222",
   "metadata": {},
   "source": [
    "## Upsert data\n",
    "\n",
    "Lets do an upsert ... this will be *upsert #1* \n",
    "\n",
    "In *upsert 1* we change **Chris** to **Chris Sharkey**\n",
    "\n",
    "also note that for this write we set [inline compaction][0] to false ```option(\"hoodie.compact.inline\", \"false\")``` . This keeps Hudi from compacting our changes during the write operation.\n",
    "\n",
    "[0]:https://hudi.apache.org/docs/0.7.0/configurations#withinlinecompactioninlinecompaction--false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b0cb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6807b556921f4e51bd9a0f7685cb40c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "        (\"1\", \"Chris Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-02 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "updateDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# Upsert the records in updateDF\n",
    "updateDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .option(\"hoodie.compact.inline\", \"false\") \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://hudi-sharkech/merge_on_read_python/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83356609",
   "metadata": {},
   "source": [
    "## Read the Hudi Table\n",
    "\n",
    "Hudi provides 3 query types\n",
    "1. Snapshot Query\n",
    "2. Read Optimized Query \n",
    "3. Incremental Query\n",
    "\n",
    "We will cover Snapshot queries and Read Optimized queries below. Incremetnal queries are covered in the [copy_on_write][1] notebooks.\n",
    "\n",
    "Query Type|Description\n",
    ":---|:---|\n",
    "Snapshot Queries|Queries that see the latest snapshot of the table as of a given commit or compaction action. For MoR tables, snapshot queries expose the most recent state of the table by merging the base and delta files of the latest file slice at the time of the query. \n",
    "Incremental Queries|Queries only see new data written to the table, since a given commit/compaction. This effectively provides change streams to enable incremental data pipelines.\n",
    "Read Optimized Queries|For MoR tables, queries see the latest data compacted. For CoW tables, queries see the latest data committed.\n",
    "\n",
    "[1]:https://github.com/ev2900/Hudi_Elastic_Map_Reduce/tree/main/copy_on_write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf970e49",
   "metadata": {},
   "source": [
    "### Snapshot Query\n",
    "\n",
    "We expect a snapshot query to return the most up to date version of a Hudi table. \n",
    "\n",
    "The snap shotquery should include *upsert 1* that changed Chris to **Chris Sharkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadcefa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1445eaf2f9664cd980e70c9c8e7cbbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-------------------+\n",
      "| id|         name|create_date|   last_update_time|\n",
      "+---+-------------+-----------+-------------------+\n",
      "|  1|Chris Sharkey| 2020-01-01|2020-01-02 00:00:00|\n",
      "|  2|         Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3|         Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4|         John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5|         Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6|         Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+-------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://hudi-sharkech/merge_on_read_python' + '/*/*')\n",
    "\n",
    "# snapshotQueryDF.orderBy(\"id\").show()\n",
    "# snapshotQueryDF.select(\"_hoodie_commit_time\", \"_hoodie_commit_seqno\", \"_hoodie_record_key\", \"_hoodie_partition_path\", \"_hoodie_file_name\").orderBy(\"_hoodie_record_key\").show()\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0891ee",
   "metadata": {},
   "source": [
    "### Read Optimized Queries\n",
    "\n",
    "A read optimized query to return the latest data compacted. \n",
    "\n",
    "*upsert 1* that changed **Chris** to **Chris Sharkey** has not been compacted to the base parquet files yet becuase we set ```option(\"hoodie.compact.inline\", \"false\")``` during the upsert in the prior step.\n",
    "\n",
    "We expect the read optimized query to **not** reflect the changes made in *upsert 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e46064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3c3575a30b45deaa13f8fc9b641f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+\n",
      "| id| name|create_date|   last_update_time|\n",
      "+---+-----+-----------+-------------------+\n",
      "|  1|Chris| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  2| Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3| Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4| John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5| Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6| Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+-----+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "readOptimizedQueryDF = spark \\\n",
    "    .read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.query.type', 'read_optimized') \\\n",
    "    .load('s3://hudi-sharkech/merge_on_read_python' + '/*/*') \\\n",
    "\n",
    "readOptimizedQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85be67",
   "metadata": {},
   "source": [
    "Now that we are getting the hang of it ... lets do another upsert this will be *upsert #2*\n",
    "\n",
    "*upsert 2* will change **Chris Sharkey** to **Chris M Sharkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea7dffc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c0cfb49636416dab10e4b910faa39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "        (\"1\", \"Chris M Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-02 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "updateDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# Upsert the records in updateDF\n",
    "updateDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .option(\"hoodie.compact.inline\", \"false\") \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://hudi-sharkech/merge_on_read_python/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b627a",
   "metadata": {},
   "source": [
    "Snapshot query ... the query results should include the changes we just made in *upsert 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a59c036d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d1ea2e2549453bb3008d4fa13724a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-----------+-------------------+\n",
      "| id|           name|create_date|   last_update_time|\n",
      "+---+---------------+-----------+-------------------+\n",
      "|  1|Chris M Sharkey| 2020-01-01|2020-01-02 00:00:00|\n",
      "|  2|           Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3|           Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4|           John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5|           Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6|           Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+---------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://hudi-sharkech/merge_on_read_python' + '/*/*')\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3759a7",
   "metadata": {},
   "source": [
    "Read optimized query .. neither *upsert 1* or *upsert 2* have been compacted yet. \n",
    "\n",
    "The read optimized query should **not** include the changes made by either *upsert 1* or *upsert 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7d91c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18f93e8988540eba059aa7a71b426cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+\n",
      "| id| name|create_date|   last_update_time|\n",
      "+---+-----+-----------+-------------------+\n",
      "|  1|Chris| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  2| Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3| Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4| John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5| Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6| Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+-----+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "readOptimizedQueryDF = spark \\\n",
    "    .read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.query.type', 'read_optimized') \\\n",
    "    .load('s3://hudi-sharkech/merge_on_read_python' + '/*/*') \\\n",
    "\n",
    "readOptimizedQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c66ca7",
   "metadata": {},
   "source": [
    "### Compaction\n",
    "\n",
    "Running a compaction will merge the changes we made in *upsert 1* and *upsert 2* with the base parquet files. After a compaction the snapshot query and read optimized query will return the same results. \n",
    "\n",
    "An easy way to trigger a compaction is to do another write operation and set ```option(\"hoodie.compact.inline\", \"true\")``` and set ```option(\"hoodie.compact.inline.max.delta.commits\", \"1\")```\n",
    "\n",
    "We could also run a compaction from the [Hudi CLI][1] or via. other methods \n",
    "\n",
    "[1]:https://hudi.apache.org/docs/0.7.0/deployment#compactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c5e56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e43e11d34248478d715f9f66fa567b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "        (\"1\", \"Christopher M Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-02 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "updateDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# Upsert the records in updateDF\n",
    "updateDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .option(\"hoodie.compact.inline\", \"true\") \\\n",
    "    .option(\"hoodie.compact.inline.max.delta.commits\", \"1\") \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://hudi-sharkech/merge_on_read_python/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b9acd",
   "metadata": {},
   "source": [
    "The snapshot query and read optimized query will should now return the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3aab60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013557853e4340d79f592d4416541f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+-------------------+\n",
      "| id|                name|create_date|   last_update_time|\n",
      "+---+--------------------+-----------+-------------------+\n",
      "|  1|Christopher M Sha...| 2020-01-01|2020-01-02 00:00:00|\n",
      "|  2|                Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3|                Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4|                John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5|                Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6|                Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+--------------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "# Snapshot query\n",
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://hudi-sharkech/merge_on_read_python' + '/*/*')\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0109b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d78cd932ef8408b832fa407340133c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+-------------------+\n",
      "| id|                name|create_date|   last_update_time|\n",
      "+---+--------------------+-----------+-------------------+\n",
      "|  1|Christopher M Sha...| 2020-01-01|2020-01-02 00:00:00|\n",
      "|  2|                Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3|                Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4|                John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5|                Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6|                Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+--------------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "# Read optimized query\n",
    "readOptimizedQueryDF = spark \\\n",
    "    .read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.query.type', 'read_optimized') \\\n",
    "    .load('s3://hudi-sharkech/merge_on_read_python' + '/*/*') \\\n",
    "\n",
    "readOptimizedQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
