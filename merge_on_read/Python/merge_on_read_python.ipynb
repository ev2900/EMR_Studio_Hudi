{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6e155b",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eebcf3b",
   "metadata": {},
   "source": [
    "Before running the code in the cell(s) below SSH into your EMR cluster and run the following \n",
    "\n",
    "```hdfs dfs -mkdir -p /apps/hudi/lib```\n",
    "\n",
    "```hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar```\n",
    "\n",
    "```hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar```\n",
    "\n",
    "This will copy the Hudi jar files from the local file system to HDFS on the master node of the notebook cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87122ffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T19:13:41.794637Z",
     "iopub.status.busy": "2022-10-06T19:13:41.794411Z",
     "iopub.status.idle": "2022-10-06T19:13:41.828135Z",
     "shell.execute_reply": "2022-10-06T19:13:41.827536Z",
     "shell.execute_reply.started": "2022-10-06T19:13:41.794613Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'proxyUser': 'assumed-role_user-role-EMR-studio_ChrisSharkey', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1665073967792_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-4.ec2.internal:20888/proxy/application_1665073967792_0003/\" class=\"emr-proxy-link j-61LTZ73BNLVH application_1665073967792_0003\" emr-resource=\"j-61LTZ73BNLVH\n\" application-id=\"application_1665073967792_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-4.ec2.internal:8042/node/containerlogs/container_1665073967792_0003_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240b404-1a7b-4a2b-a60e-c8b8e8d63f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T18:53:51.638051Z",
     "iopub.status.busy": "2022-10-06T18:53:51.637831Z",
     "iopub.status.idle": "2022-10-06T18:53:51.709204Z",
     "shell.execute_reply": "2022-10-06T18:53:51.708394Z",
     "shell.execute_reply.started": "2022-10-06T18:53:51.638026Z"
    }
   },
   "source": [
    "Set a variable equal to the name of the S3 bucket to read / write from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8954ce06-7a8b-48ed-b489-ecf42f25ff25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T19:13:43.716754Z",
     "iopub.status.busy": "2022-10-06T19:13:43.716528Z",
     "iopub.status.idle": "2022-10-06T19:14:15.370760Z",
     "shell.execute_reply": "2022-10-06T19:14:15.370149Z",
     "shell.execute_reply.started": "2022-10-06T19:13:43.716730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b9c4a827374572acee2137f2b9fc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>7</td><td>application_1665073967792_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-4.ec2.internal:20888/proxy/application_1665073967792_0008/\" class=\"emr-proxy-link j-61LTZ73BNLVH application_1665073967792_0008\" emr-resource=\"j-61LTZ73BNLVH\n\" application-id=\"application_1665073967792_0008\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-4.ec2.internal:8042/node/containerlogs/container_1665073967792_0008_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_bucket_name = \"emr-studio-demo-s3bucket-g3frvpeeuanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a7f2d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T19:14:15.372506Z",
     "iopub.status.busy": "2022-10-06T19:14:15.372257Z",
     "iopub.status.idle": "2022-10-06T19:14:15.445601Z",
     "shell.execute_reply": "2022-10-06T19:14:15.444863Z",
     "shell.execute_reply.started": "2022-10-06T19:14:15.372472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579bad8bd5a64c88a649c6268589af60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0bd93",
   "metadata": {},
   "source": [
    "## Write to S3 via. Hudi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea975cac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T19:14:15.447276Z",
     "iopub.status.busy": "2022-10-06T19:14:15.447104Z",
     "iopub.status.idle": "2022-10-06T19:14:54.893633Z",
     "shell.execute_reply": "2022-10-06T19:14:54.892846Z",
     "shell.execute_reply.started": "2022-10-06T19:14:15.447255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb6a4b266c84b23bc248db16ddfc5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "        (\"1\", \"Chris\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"2\", \"Will\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"3\", \"Emma\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"4\", \"John\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"5\", \"Eric\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"6\", \"Adam\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False), \n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "inputDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# Create hudiOptions variable\n",
    "hudiOptions = {\n",
    "    'hoodie.table.name': 'copy_on_write_python',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'create_date',\n",
    "    'hoodie.datasource.write.precombine.field': 'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.enable': 'false',\n",
    "    'hoodie.datasource.hive_sync.table': 'copy_on_write_python',\n",
    "    'hoodie.datasource.hive_sync.partition_fields': 'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "    'hoodie.datasource.write.table.type': 'MERGE_ON_READ'\n",
    "}\n",
    "\n",
    "# Write a DataFrame to S3 as a Hudi dataset \n",
    "inputDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'insert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac5222",
   "metadata": {},
   "source": [
    "## Upsert data\n",
    "\n",
    "Lets do an upsert ... this will be *upsert #1* \n",
    "\n",
    "In *upsert 1* we change **Chris** to **Chris Sharkey**\n",
    "\n",
    "also note that for this write we set [inline compaction][0] to false ```option(\"hoodie.compact.inline\", \"false\")``` . This keeps Hudi from compacting our changes during the write operation.\n",
    "\n",
    "[0]:https://hudi.apache.org/docs/0.7.0/configurations#withinlinecompactioninlinecompaction--false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b0cb47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T19:14:54.895096Z",
     "iopub.status.busy": "2022-10-06T19:14:54.894932Z",
     "iopub.status.idle": "2022-10-06T19:15:26.290115Z",
     "shell.execute_reply": "2022-10-06T19:15:26.289268Z",
     "shell.execute_reply.started": "2022-10-06T19:14:54.895075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05338c0e35fb45a19033c5f8f5aadfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "        (\"1\", \"Chris Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-02 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "updateDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# Upsert the records in updateDF\n",
    "updateDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .option(\"hoodie.compact.inline\", \"false\") \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83356609",
   "metadata": {},
   "source": [
    "## Read the Hudi Table\n",
    "\n",
    "Hudi provides 3 query types\n",
    "1. Snapshot Query\n",
    "2. Read Optimized Query \n",
    "3. Incremental Query\n",
    "\n",
    "We will cover Snapshot queries and Read Optimized queries below. Incremetnal queries are covered in the [copy_on_write][1] notebooks.\n",
    "\n",
    "Query Type|Description\n",
    ":---|:---|\n",
    "Snapshot Queries|Queries that see the latest snapshot of the table as of a given commit or compaction action. For MoR tables, snapshot queries expose the most recent state of the table by merging the base and delta files of the latest file slice at the time of the query. \n",
    "Incremental Queries|Queries only see new data written to the table, since a given commit/compaction. This effectively provides change streams to enable incremental data pipelines.\n",
    "Read Optimized Queries|For MoR tables, queries see the latest data compacted. For CoW tables, queries see the latest data committed.\n",
    "\n",
    "[1]:https://github.com/ev2900/Hudi_Elastic_Map_Reduce/tree/main/copy_on_write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf970e49",
   "metadata": {},
   "source": [
    "### Snapshot Query\n",
    "\n",
    "We expect a snapshot query to return the most up to date version of a Hudi table. \n",
    "\n",
    "The snap shotquery should include *upsert 1* that changed Chris to **Chris Sharkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dadcefa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T19:15:26.291596Z",
     "iopub.status.busy": "2022-10-06T19:15:26.291316Z",
     "iopub.status.idle": "2022-10-06T19:15:31.611853Z",
     "shell.execute_reply": "2022-10-06T19:15:31.610736Z",
     "shell.execute_reply.started": "2022-10-06T19:15:26.291560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec4b8122c7b42b78fd95f227610be48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-------------------+\n",
      "| id|         name|create_date|   last_update_time|\n",
      "+---+-------------+-----------+-------------------+\n",
      "|  1|Chris Sharkey| 2020-01-01|2020-01-02 00:00:00|\n",
      "|  2|         Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3|         Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4|         John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5|         Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6|         Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+-------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/*/*')\n",
    "\n",
    "# snapshotQueryDF.orderBy(\"id\").show()\n",
    "# snapshotQueryDF.select(\"_hoodie_commit_time\", \"_hoodie_commit_seqno\", \"_hoodie_record_key\", \"_hoodie_partition_path\", \"_hoodie_file_name\").orderBy(\"_hoodie_record_key\").show()\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0891ee",
   "metadata": {},
   "source": [
    "### Read Optimized Queries\n",
    "\n",
    "A read optimized query to return the latest data compacted. \n",
    "\n",
    "*upsert 1* that changed **Chris** to **Chris Sharkey** has not been compacted to the base parquet files yet becuase we set ```option(\"hoodie.compact.inline\", \"false\")``` during the upsert in the prior step.\n",
    "\n",
    "We expect the read optimized query to **not** reflect the changes made in *upsert 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e46064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T19:15:31.613538Z",
     "iopub.status.busy": "2022-10-06T19:15:31.613267Z",
     "iopub.status.idle": "2022-10-06T19:15:34.972352Z",
     "shell.execute_reply": "2022-10-06T19:15:34.971489Z",
     "shell.execute_reply.started": "2022-10-06T19:15:31.613501Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a7b44f4a47437095058b3f249ca06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+\n",
      "| id| name|create_date|   last_update_time|\n",
      "+---+-----+-----------+-------------------+\n",
      "|  1|Chris| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  2| Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3| Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4| John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5| Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6| Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+-----+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "readOptimizedQueryDF = spark \\\n",
    "    .read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.query.type', 'read_optimized') \\\n",
    "    .load('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/*/*') \\\n",
    "\n",
    "readOptimizedQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85be67",
   "metadata": {},
   "source": [
    "Now that we are getting the hang of it ... lets do another upsert this will be *upsert #2*\n",
    "\n",
    "*upsert 2* will change **Chris Sharkey** to **Chris M Sharkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7dffc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-06T19:15:34.974067Z",
     "iopub.status.busy": "2022-10-06T19:15:34.973609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e57a420bc84999b097c876242b108e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afefd034893949828aee8fd0283e972d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "        (\"1\", \"Chris M Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-02 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "updateDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# Upsert the records in updateDF\n",
    "updateDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .option(\"hoodie.compact.inline\", \"false\") \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b627a",
   "metadata": {},
   "source": [
    "Snapshot query ... the query results should include the changes we just made in *upsert 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/*/*')\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3759a7",
   "metadata": {},
   "source": [
    "Read optimized query .. neither *upsert 1* or *upsert 2* have been compacted yet. \n",
    "\n",
    "The read optimized query should **not** include the changes made by either *upsert 1* or *upsert 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "readOptimizedQueryDF = spark \\\n",
    "    .read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.query.type', 'read_optimized') \\\n",
    "    .load('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/*/*') \\\n",
    "\n",
    "readOptimizedQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c66ca7",
   "metadata": {},
   "source": [
    "### Compaction\n",
    "\n",
    "Running a compaction will merge the changes we made in *upsert 1* and *upsert 2* with the base parquet files. After a compaction the snapshot query and read optimized query will return the same results. \n",
    "\n",
    "An easy way to trigger a compaction is to do another write operation and set ```option(\"hoodie.compact.inline\", \"true\")``` and set ```option(\"hoodie.compact.inline.max.delta.commits\", \"1\")```\n",
    "\n",
    "We could also run a compaction from the [Hudi CLI][1] or via. other methods \n",
    "\n",
    "[1]:https://hudi.apache.org/docs/0.7.0/deployment#compactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "        (\"1\", \"Christopher M Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-02 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "updateDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# Upsert the records in updateDF\n",
    "updateDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .option(\"hoodie.compact.inline\", \"true\") \\\n",
    "    .option(\"hoodie.compact.inline.max.delta.commits\", \"1\") \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b9acd",
   "metadata": {},
   "source": [
    "The snapshot query and read optimized query will should now return the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aab60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Snapshot query\n",
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/*/*')\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0109b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read optimized query\n",
    "readOptimizedQueryDF = spark \\\n",
    "    .read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.query.type', 'read_optimized') \\\n",
    "    .load('s3://' + s3_bucket_name + '/hudi/merge_on_read_python/*/*') \\\n",
    "\n",
    "readOptimizedQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
