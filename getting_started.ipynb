{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d14e55f",
   "metadata": {},
   "source": [
    "## Reference Material\n",
    "\n",
    "* [Hudi Documentation Quick Start Guide][1]\n",
    "* [EMR Hudi Documentation][2]\n",
    "* [EMR Hudi Docuementation - Work with a Hudi dataset][3]\n",
    "\n",
    "[1]:https://hudi.apache.org/docs/quick-start-guide/#setup]\n",
    "[2]:https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi.html\n",
    "[3]:https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1b59d",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aebd8e",
   "metadata": {},
   "source": [
    "Before running the code in the cell(s) below SSH into your EMR cluster and run the following \n",
    "\n",
    "```hdfs dfs -mkdir -p /apps/hudi/lib```\n",
    "\n",
    "```hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar```\n",
    "\n",
    "```hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar```\n",
    "\n",
    "This will copy the Hudi jar files from the local file system to HDFS on the master node of the notebook cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d8fc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'proxyUser': 'assumed-role_admin_sharkech-Isengard', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c1cf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5f8663306d4598b9c689106f8b3fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1636114111194_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-42-160.ec2.internal:20888/proxy/application_1636114111194_0002/\" class=\"emr-proxy-link\" emr-resource=\"j-13ZMAYPSAIJP8\n",
       "\" application-id=\"application_1636114111194_0002\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-46-22.ec2.internal:8042/node/containerlogs/container_1636114111194_0002_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3325950",
   "metadata": {},
   "source": [
    "## Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f40e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4771b2af9f44b6b9d6e0cf756942578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "        (\"1\", \"Chris\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"2\", \"Will\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"3\", \"Emma\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"4\", \"John\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"5\", \"Eric\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"6\", \"Adam\", \"2020-01-01\", datetime.strptime('2020-01-01 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False), \n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "inputDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# inputDF.show()\n",
    "# inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3baccce",
   "metadata": {},
   "source": [
    "## Write to S3 via. Hudi\n",
    "\n",
    "We create a ```hudiOptions``` variable. We use this when we write data to S3. \n",
    "\n",
    "DataSourceWriteOptions for ***Hudi***: \n",
    "\n",
    "Option|Description\n",
    ":---|:---|\n",
    "TABLE_NAME|The table name under which to register the dataset\n",
    "TABLE_TYPE_OPT_KEY|Optional. Specifies whether the dataset is created as ```COPY_ON_WRITE``` or ```MERGE_ON_READ```. The default is ```COPY_ON_WRITE```\n",
    "RECORDKEY_FIELD_OPT_KEY|The record key field whose value will be used as the recordKey component of HoodieKey. Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using the dot notation, for example, a.b.c\n",
    "PARTITIONPATH_FIELD_OPT_KEY|The partition path field whose value will be used as the partitionPath component of HoodieKey. The actual value will be obtained by invoking .toString() on the field value\n",
    "PRECOMBINE_FIELD_OPT_KEY|The field used in pre-combining before actual write. When two records have the same key value, Hudi picks the one with the largest value for the precombine field as determined by Object.compareTo(..)\n",
    "\n",
    "DataSourceWriteOptions for ***Hive***:\n",
    "\n",
    "Option|Description\n",
    ":---|:---|\n",
    "HIVE_DATABASE_OPT_KEY|The Hive database to sync to. The default is ```default```\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY|The class used to extract partition field values into Hive partition columns\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY|The field in the dataset to use for determining Hive partition columns\n",
    "HIVE_SYNC_ENABLED_OPT_KEY|When set to ```true```, registers the dataset with the Apache Hive metastore.                     \n",
    "HIVE_TABLE_OPT_KEY|Required. The name of the table in Hive to sync to. For example the table name can be, ```my_hudi_table``` or any other name specified\n",
    "HIVE_USER_OPT_KEY|Optional. The Hive user name to use when syncing. For example, ```hadoop```\n",
    "HIVE_PASS_OPT_KEY|Optional. The Hive password for the user specified by HIVE_USER_OPT_KEY\n",
    "HIVE_URL_OPT_KEY|The Hive metastore URL\n",
    "\n",
    "*ensure that the ```hoodie.datasource.write.partitionpath.field``` and ```hoodie.datasource.write.precombine.field``` use different columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f13fa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e22432959a44289b5aaf557bc51cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create hudiOptions variable\n",
    "hudiOptions = {\n",
    "    'hoodie.table.name': 'my_hudi_table',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'create_date',\n",
    "    'hoodie.datasource.write.precombine.field': 'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.enable': 'true',\n",
    "    'hoodie.datasource.hive_sync.table': 'my_hudi_table',\n",
    "    'hoodie.datasource.hive_sync.partition_fields': 'last_update_time',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd633a",
   "metadata": {},
   "source": [
    "*Note* adjust the s3 path in ```.save()```\n",
    "\n",
    "Options for ```hoodie.datasource.write.operation```\n",
    "\n",
    "Option|Description\n",
    ":---|:---|\n",
    "UPSERT|This is the default operation where the input records are first tagged as inserts or updates by looking up the index. The records are ultimately written after heuristics are run to determine how best to pack them on storage to optimize for things like file sizing. This operation is recommended for use-cases like database change capture where the input almost certainly contains updates. The target table will never show duplicates\n",
    "INSERT|This operation is very similar to upsert in terms of heuristics/file sizing but completely skips the index lookup step. Thus, it can be a lot faster than upserts for use-cases like log de-duplication (in conjunction with options to filter duplicates mentioned below). This is also suitable for use-cases where the table can tolerate duplicates, but just need the transactional writes/incremental pull/storage management capabilities of Hudi\n",
    "BULK_INSERT|Both upsert and insert operations keep input records in memory to speed up storage heuristics computations faster (among other things) and thus can be cumbersome for initial loading/bootstrapping a Hudi table at first. Bulk insert provides the same semantics as insert, while implementing a sort-based data writing algorithm, which can scale very well for several hundred TBs of initial load. However, this just does a best-effort job at sizing files vs guaranteeing file sizes like inserts/upserts do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed78a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe708e44a564ea7bd7e98a46d1c2add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write a DataFrame to S3 as a Hudi dataset \n",
    "inputDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'insert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('s3://hudi-sharkech/myhudidataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff6a92",
   "metadata": {},
   "source": [
    "## Read the Hudi Table\n",
    "\n",
    "Hudi performs snapshot queries by default. Snapshot queries retrieve data at the present point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d071159a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8782aad3115f46748699ccc22979867f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+\n",
      "| id| name|create_date|   last_update_time|\n",
      "+---+-----+-----------+-------------------+\n",
      "|  1|Chris| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  2| Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3| Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4| John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5| Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6| Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "+---+-----+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://hudi-sharkech/myhudidataset' + '/*/*')\n",
    "\n",
    "# snapshotQueryDF.orderBy(\"id\").show()\n",
    "# snapshotQueryDF.select(\"_hoodie_commit_time\", \"_hoodie_commit_seqno\", \"_hoodie_record_key\", \"_hoodie_partition_path\", \"_hoodie_file_name\").orderBy(\"_hoodie_record_key\").show()\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674f261",
   "metadata": {},
   "source": [
    "## Upsert data\n",
    "\n",
    "Lets do an upsert ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3df804d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f68c9e70d7c417ea7d31efcbe65082f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new data frame with an updated last_update_time\n",
    "data = [\n",
    "        (\"1\", \"Chris Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-02 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"7\", \"Kelly\", \"2020-01-02\", datetime.strptime('2020-01-02 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "updateDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# updateDF.show()\n",
    "# updateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223a05ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9797b49dd04b1ca3dc8fb105b5559a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upsert the records in updateDF\n",
    "updateDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://hudi-sharkech/myhudidataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddfeab76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2d07e6555e48aeb8168f6de4251cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-------------------+\n",
      "| id|         name|create_date|   last_update_time|\n",
      "+---+-------------+-----------+-------------------+\n",
      "|  1|Chris Sharkey| 2020-01-01|2020-01-02 00:00:00|\n",
      "|  2|         Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3|         Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4|         John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5|         Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6|         Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  7|        Kelly| 2020-01-02|2020-01-02 00:00:00|\n",
      "+---+-------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "# Check that the upsert worked\n",
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://hudi-sharkech/myhudidataset' + '/*/*')\n",
    "\n",
    "# snapshotQueryDF.show()\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f77f7",
   "metadata": {},
   "source": [
    "Lets do another upsert ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e41dafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b350ed746664e42b21fd186b6d15bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new data frame with an updated last_update_time\n",
    "data = [\n",
    "        (\"1\", \"Christopher Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-03 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"8\", \"Ella\", \"2020-01-03\", datetime.strptime('2020-01-03 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "updateDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# updateDF.show()\n",
    "# updateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e165942a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4f5793dec34098acf7ac6f7d4795ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upsert the records in updateDF\n",
    "updateDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .option('hoodie.datasource.write.operation', 'upsert') \\\n",
    "    .options(**hudiOptions) \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://hudi-sharkech/myhudidataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38775db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910a56e86b624596bc12b2d8f316a49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+-------------------+\n",
      "| id|               name|create_date|   last_update_time|\n",
      "+---+-------------------+-----------+-------------------+\n",
      "|  1|Christopher Sharkey| 2020-01-01|2020-01-03 00:00:00|\n",
      "|  2|               Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3|               Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4|               John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5|               Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6|               Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  7|              Kelly| 2020-01-02|2020-01-02 00:00:00|\n",
      "|  8|               Ella| 2020-01-03|2020-01-03 00:00:00|\n",
      "+---+-------------------+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "# Check that the upsert worked\n",
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://hudi-sharkech/myhudidataset' + '/*/*')\n",
    "\n",
    "# snapshotQueryDF.show()\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17685eb7",
   "metadata": {},
   "source": [
    "## Delete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b17f5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bd0b700d0241d08a2291c6c08ef7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new data frame\n",
    "data = [\n",
    "        (\"1\", \"Christopher Sharkey\", \"2020-01-01\", datetime.strptime('2020-01-03 00:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "        (\"8\", \"Ella\", \"2020-01-03\", datetime.strptime('2020-01-03 00:00:00', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"create_date\", StringType(), False),             \n",
    "        StructField(\"last_update_time\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "deleteDF = spark.createDataFrame(data=data,schema=schema)\n",
    "\n",
    "# updateDF.show()\n",
    "# updateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "402cf2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5defc2ac20234bafa1cf3c7dee0b1cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create hudiOptions variable\n",
    "hudiOptionsDelete = {\n",
    "    'hoodie.table.name': 'my_hudi_table',\n",
    "    'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'create_date',\n",
    "    'hoodie.datasource.write.precombine.field': 'last_update_time',\n",
    "    'hoodie.datasource.write.operation': 'delete'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc114728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f7354caabc4aa1991c31031f7f09ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deleteDF \\\n",
    "    .write \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .options(**hudiOptionsDelete) \\\n",
    "    .mode('append') \\\n",
    "    .save('s3://hudi-sharkech/myhudidataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7be5a51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837a37469a8143ca85bb9892f38a33fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+-------------------+\n",
      "| id| name|create_date|   last_update_time|\n",
      "+---+-----+-----------+-------------------+\n",
      "|  2| Will| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  3| Emma| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  4| John| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  5| Eric| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  6| Adam| 2020-01-01|2020-01-01 00:00:00|\n",
      "|  7|Kelly| 2020-01-02|2020-01-02 00:00:00|\n",
      "+---+-----+-----------+-------------------+"
     ]
    }
   ],
   "source": [
    "# Check that the upsert worked\n",
    "snapshotQueryDF = spark.read.format('org.apache.hudi').load('s3://hudi-sharkech/myhudidataset' + '/*/*')\n",
    "\n",
    "# snapshotQueryDF.show()\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9cfef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
