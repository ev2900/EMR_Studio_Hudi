{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319c5d5d",
   "metadata": {},
   "source": [
    "## Reference Material\n",
    "\n",
    "* [Hudi Documentation Quick Start Guide][1]\n",
    "* [EMR Hudi Documentation][2]\n",
    "* [EMR Hudi Docuementation - Work with a Hudi dataset][3]\n",
    "\n",
    "[1]:https://hudi.apache.org/docs/quick-start-guide/#setup]\n",
    "[2]:https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi.html\n",
    "[3]:https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac7a43",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698e1c4",
   "metadata": {},
   "source": [
    "Before running the code in the cell(s) below SSH into your EMR cluster and run the following \n",
    "\n",
    "```hdfs dfs -mkdir -p /apps/hudi/lib```\n",
    "\n",
    "```hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar```\n",
    "\n",
    "```hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar```\n",
    "\n",
    "This will copy the Hudi jar files from the local file system to HDFS on the master node of the notebook cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6408b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'proxyUser': 'assumed-role_admin_sharkech-Isengard', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94a71e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0070b2871f5e4988be761baa208a5333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1636465466779_0009</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-43-207.ec2.internal:20888/proxy/application_1636465466779_0009/\" class=\"emr-proxy-link\" emr-resource=\"j-1JETVFHCNDPKD\n",
       "\" application-id=\"application_1636465466779_0009\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-41-193.ec2.internal:8042/node/containerlogs/container_1636465466779_0009_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.hudi.DataSourceWriteOptions\n",
      "import org.apache.hudi.DataSourceReadOptions\n",
      "import org.apache.hudi.config.HoodieWriteConfig\n",
      "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "import java.sql.Timestamp\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.hudi.DataSourceWriteOptions\n",
    "import org.apache.hudi.DataSourceReadOptions\n",
    "import org.apache.hudi.config.HoodieWriteConfig\n",
    "import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2abfb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bc5c3249e041879f892f830ee19126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputDF: org.apache.spark.sql.DataFrame = [id: string, name: string ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "val inputDF = Seq(\n",
    "    (\"1\", \"Chris\", \"2020-01-01\", Timestamp.valueOf(\"2020-01-01 00:00:00\")),\n",
    "    (\"2\", \"Will\", \"2020-01-01\", Timestamp.valueOf(\"2020-01-01 00:00:00\")),\n",
    "    (\"3\", \"Emma\", \"2020-01-01\", Timestamp.valueOf(\"2020-01-01 00:00:00\")),\n",
    "    (\"4\", \"John\", \"2020-01-01\", Timestamp.valueOf(\"2020-01-01 00:00:00\")),\n",
    "    (\"5\", \"Eric\", \"2020-01-01\", Timestamp.valueOf(\"2020-01-01 00:00:00\")),\n",
    "    (\"6\", \"Adam\", \"2020-01-01\", Timestamp.valueOf(\"2020-01-01 00:00:00\"))\n",
    ").toDF(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"create_date\",\n",
    "    \"last_update_time\"\n",
    ")\n",
    "\n",
    "// inputDF.show()\n",
    "// inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd9518",
   "metadata": {},
   "source": [
    "## Write to S3 via. Hudi\n",
    "\n",
    "We create a ```hudiOptions``` variable. We use this when we write data to S3. \n",
    "\n",
    "DataSourceWriteOptions for ***Hudi***: \n",
    "\n",
    "Option|Description\n",
    ":---|:---|\n",
    "TABLE_NAME|The table name under which to register the dataset\n",
    "TABLE_TYPE_OPT_KEY|Optional. Specifies whether the dataset is created as ```COPY_ON_WRITE``` or ```MERGE_ON_READ```. The default is ```COPY_ON_WRITE```\n",
    "RECORDKEY_FIELD_OPT_KEY|The record key field whose value will be used as the recordKey component of HoodieKey. Actual value will be obtained by invoking .toString() on the field value. Nested fields can be specified using the dot notation, for example, a.b.c\n",
    "PARTITIONPATH_FIELD_OPT_KEY|The partition path field whose value will be used as the partitionPath component of HoodieKey. The actual value will be obtained by invoking .toString() on the field value\n",
    "PRECOMBINE_FIELD_OPT_KEY|The field used in pre-combining before actual write. When two records have the same key value, Hudi picks the one with the largest value for the precombine field as determined by Object.compareTo(..)\n",
    "\n",
    "DataSourceWriteOptions for ***Hive***:\n",
    "\n",
    "Option|Description\n",
    ":---|:---|\n",
    "HIVE_DATABASE_OPT_KEY|The Hive database to sync to. The default is ```default```\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY|The class used to extract partition field values into Hive partition columns\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY|The field in the dataset to use for determining Hive partition columns\n",
    "HIVE_SYNC_ENABLED_OPT_KEY|When set to ```true```, registers the dataset with the Apache Hive metastore.                     \n",
    "HIVE_TABLE_OPT_KEY|Required. The name of the table in Hive to sync to. For example the table name can be, ```my_hudi_table``` or any other name can be specified for the value of the Hive Table Opt Key Hudi table name\n",
    "HIVE_USER_OPT_KEY|Optional. The Hive user name to use when syncing. For example, ```hadoop```\n",
    "HIVE_PASS_OPT_KEY|Optional. The Hive password for the user specified by HIVE_USER_OPT_KEY\n",
    "HIVE_URL_OPT_KEY|The Hive metastore URL\n",
    "\n",
    "For a full list of configurations view the [Hudi Documentation - Configurations][1]\n",
    "\n",
    "*ensure that the ```DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY``` and ```DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY``` use different columns*\n",
    "\n",
    "[1]:https://hudi.apache.org/docs/configurations/#Index-Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058c8a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2bb673c5ae42d195315e8413d5ec93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hudiOptions: scala.collection.immutable.Map[String,String] = Map(hoodie.datasource.write.precombine.field -> last_update_time, hoodie.datasource.hive_sync.partition_fields -> creation_date, hoodie.datasource.hive_sync.partition_extractor_class -> org.apache.hudi.hive.MultiPartKeysValueExtractor, hoodie.datasource.hive_sync.table -> my_hudi_table, hoodie.datasource.hive_sync.enable -> true, hoodie.datasource.write.recordkey.field -> id, hoodie.table.name -> my_hudi_table_scala, hoodie.datasource.write.table.type -> COPY_ON_WRITE, hoodie.datasource.write.partitionpath.field -> creation_date)\n"
     ]
    }
   ],
   "source": [
    "// Create hudiOptions variable\n",
    "val hudiOptions = Map[String,String](\n",
    "  HoodieWriteConfig.TABLE_NAME -> \"my_hudi_table_scala\",\n",
    "  DataSourceWriteOptions.TABLE_TYPE_OPT_KEY -> \"COPY_ON_WRITE\", \n",
    "  DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"id\",\n",
    "  DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"creation_date\",\n",
    "  DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"last_update_time\",\n",
    "  DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY -> \"true\",\n",
    "  DataSourceWriteOptions.HIVE_TABLE_OPT_KEY -> \"my_hudi_table\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"creation_date\",\n",
    "  DataSourceWriteOptions.HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY -> classOf[MultiPartKeysValueExtractor].getName\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404d377",
   "metadata": {},
   "source": [
    "*Note* adjust the s3 path in ```.save()```\n",
    "\n",
    "Options for ```DataSourceWriteOptions```\n",
    "\n",
    "Option|Description\n",
    ":---|:---|\n",
    "UPSERT_OPERATION_OPT_VAL|This is the default operation where the input records are first tagged as inserts or updates by looking up the index. The records are ultimately written after heuristics are run to determine how best to pack them on storage to optimize for things like file sizing. This operation is recommended for use-cases like database change capture where the input almost certainly contains updates. The target table will never show duplicates\n",
    "INSERT_OPERATION_OPT_VAL|This operation is very similar to upsert in terms of heuristics/file sizing but completely skips the index lookup step. Thus, it can be a lot faster than upserts for use-cases like log de-duplication (in conjunction with options to filter duplicates mentioned below). This is also suitable for use-cases where the table can tolerate duplicates, but just need the transactional writes/incremental pull/storage management capabilities of Hudi\n",
    "BULK_INSERT_OPERATION_OPT_VAL|Both upsert and insert operations keep input records in memory to speed up storage heuristics computations faster (among other things) and thus can be cumbersome for initial loading/bootstrapping a Hudi table at first. Bulk insert provides the same semantics as insert, while implementing a sort-based data writing algorithm, which can scale very well for several hundred TBs of initial load. However, this just does a best-effort job at sizing files vs guaranteeing file sizes like inserts/upserts do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db43c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819020d47d764aeb9706b9151ee4a9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Write the DataFrame as a Hudi dataset\n",
    "inputDF.write.format(\"org.apache.hudi\").option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL).options(hudiOptions).mode(SaveMode.Overwrite).save(\"s3://hudi-sharkech/myhudidataset_scala/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63798cf",
   "metadata": {},
   "source": [
    "## Read the Hudi Table\n",
    "\n",
    "Hudi performs snapshot queries by default. Snapshot queries retrieve data at the present point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169c5c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ef31c6ad254282976644e7338ce5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snapshotQueryDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 7 more fields]\n",
      "+---+------------------+-------------------+--------------------+----------------------+--------------------+\n",
      "| id|_hoodie_record_key|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_partition_path|   _hoodie_file_name|\n",
      "+---+------------------+-------------------+--------------------+----------------------+--------------------+\n",
      "|  1|                 1|     20211110133114|  20211110133114_0_1|               default|2526da78-2eaa-448...|\n",
      "|  2|                 2|     20211110133114|  20211110133114_0_4|               default|2526da78-2eaa-448...|\n",
      "|  3|                 3|     20211110133114|  20211110133114_0_2|               default|2526da78-2eaa-448...|\n",
      "|  4|                 4|     20211110133114|  20211110133114_0_5|               default|2526da78-2eaa-448...|\n",
      "|  5|                 5|     20211110133114|  20211110133114_0_3|               default|2526da78-2eaa-448...|\n",
      "|  6|                 6|     20211110133114|  20211110133114_0_6|               default|2526da78-2eaa-448...|\n",
      "+---+------------------+-------------------+--------------------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val snapshotQueryDF = spark.read.format(\"org.apache.hudi\").load(\"s3://hudi-sharkech/myhudidataset_scala\" + \"/*/*\")\n",
    "\n",
    "// snapshotQueryDF.orderBy(\"id\").show()\n",
    "snapshotQueryDF.select(\"id\", \"_hoodie_record_key\", \"_hoodie_commit_time\", \"_hoodie_commit_seqno\", \"_hoodie_partition_path\", \"_hoodie_file_name\").orderBy(\"id\").show()\n",
    "\n",
    "// snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0bea2",
   "metadata": {},
   "source": [
    "## Upsert data\n",
    "\n",
    "Lets do an upsert ... this will be *upsert #1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57dd93c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4a03f3727e48b688e5e69fcb63557d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updateDF: org.apache.spark.sql.DataFrame = [id: string, name: string ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "val updateDF = Seq(\n",
    "    (\"1\", \"Chris Sharkey\", \"2020-01-01\", Timestamp.valueOf(\"2020-01-02 00:00:00\")),\n",
    "    (\"7\", \"Kelly\", \"2020-01-02\", Timestamp.valueOf(\"2020-01-02 00:00:00\"))\n",
    ").toDF(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"create_date\",\n",
    "    \"last_update_time\"\n",
    ")\n",
    "\n",
    "// inputDF.show()\n",
    "// inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe47e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1958d1e23e6746618cb537968d88f662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1829153e97c14d82aa13f9c0ce7d8431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Upsert the records in updateDF\n",
    "updateDF.write.format(\"org.apache.hudi\").option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL).options(hudiOptions).mode(SaveMode.Append).save(\"s3://hudi-sharkech/myhudidataset_scala/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Check that the upsert worked\n",
    "val snapshotQueryDF = spark.read.format(\"org.apache.hudi\").load(\"s3://hudi-sharkech/myhudidataset_scala\" + \"/*/*\")\n",
    "\n",
    "// snapshotQueryDF.orderBy(\"id\").show()\n",
    "// snapshotQueryDF.select(\"id\", \"_hoodie_record_key\", \"_hoodie_commit_time\", \"_hoodie_commit_seqno\", \"_hoodie_partition_path\", \"_hoodie_file_name\").orderBy(\"id\").show()\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93bbb0",
   "metadata": {},
   "source": [
    "Lets do another upsert ... this will be *upsert #2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea45356",
   "metadata": {},
   "outputs": [],
   "source": [
    "val updateDF = Seq(\n",
    "    (\"1\", \"Christopher Sharkey\", \"2020-01-01\", Timestamp.valueOf(\"2020-01-03 00:00:00\")),\n",
    "    (\"8\", \"Ella\", \"2020-01-03\", Timestamp.valueOf(\"2020-01-03 00:00:00\"))\n",
    ").toDF(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"create_date\",\n",
    "    \"last_update_time\"\n",
    ")\n",
    "\n",
    "// inputDF.show()\n",
    "// inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb63bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Upsert the records in updateDF\n",
    "updateDF.write.format(\"org.apache.hudi\").option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.UPSERT_OPERATION_OPT_VAL).options(hudiOptions).mode(SaveMode.Append).save(\"s3://hudi-sharkech/myhudidataset_scala/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Check that the upsert worked\n",
    "val snapshotQueryDF = spark.read.format(\"org.apache.hudi\").load(\"s3://hudi-sharkech/myhudidataset_scala\" + \"/*/*\")\n",
    "\n",
    "// snapshotQueryDF.orderBy(\"id\").show()\n",
    "// snapshotQueryDF.select(\"id\", \"_hoodie_record_key\", \"_hoodie_commit_time\", \"_hoodie_commit_seqno\", \"_hoodie_partition_path\", \"_hoodie_file_name\").orderBy(\"id\").show()\n",
    "\n",
    "snapshotQueryDF.select(\"id\", \"name\", \"create_date\", \"last_update_time\").orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5bcd5",
   "metadata": {},
   "source": [
    "## Incremental query\n",
    "\n",
    "So far we have preformed 3 actions on our Hudi table \n",
    "\n",
    "1. Inital write to the Hudi table ```myhudidataset```\n",
    "2. *Upsert 1* - Changed **Chris** to **Chris Sharkey** & added a new record for **Kelly**\n",
    "3. *Upsert 2*  - Changed **Chris Sharkey** to **Christopher Sharkey** & added a new record for **Ella**\n",
    "\n",
    "We performed each of the 3 actions seperatly aka. in 3 Hudi commits \n",
    "\n",
    "Lets look at the distinct commit times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "// View the commits times\n",
    "val snapshotQueryDF = spark.read.format(\"org.apache.hudi\").load(\"s3://hudi-sharkech/myhudidataset_scala\" + \"/*/*\")\n",
    "\n",
    "snapshotQueryDF.select(\"_hoodie_commit_time\").orderBy(\"_hoodie_commit_time\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c506b8",
   "metadata": {},
   "source": [
    "notice that we have 3 unique commit times corresponding to the 3 commits we have performed so far\n",
    "\n",
    "Hudi provides a query type ```incremental``` which can identify all of the records that have changed between given commit times\n",
    "\n",
    "The example below will identify all of the records that have changed since our first commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747bcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
